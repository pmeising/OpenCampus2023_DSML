---
title: "R Notebook"
output: html_notebook
---

```{r include=FALSE}
# Create list with required libraries
libraries <- c("dplyr", "readr", "reticulate", "ggplot2", "Metrics", "tidyverse", "glmnet", "e1071", "lubridate", "keras")


# Loop through each library and check if it's installed, if not, install it and then load it to include in this project
for (lib in libraries)
{
  if (!require(lib, character.only = TRUE))
  {
    install.packages(lib)
    library(lib, character.only = TRUE)
  }
}
```

The following piece of code does not have to be run repeatedly, only initial run is required.
```{r include=FALSE}

# Custom function to check if a Conda environment exists
#condaenv_exists <- function(env_name) {
#  conda_envs <- conda_list()
#  return(env_name %in% conda_envs$name)
#}

# Check if conda is installed, if not, install miniconda
#conda_envs <- conda_list()
#if (length(conda_envs) == 0) {
#  install_miniconda()
#}

# Create a specific Python environment if it doesn't exist
#if (!condaenv_exists("r-reticulate")) {
#  conda_create("r-reticulate", python_version = "3.8")
#}

# Get the list of installed packages in the created environment
# conda_envs <- conda_list()
# r_reticulate_env <- conda_envs[conda_envs$name == "r-reticulate", ]
# installed_packages <- r_reticulate_env$packages

# Install required packages in the created environment
# required_packages <- c("pandas", "numpy", "tensorflow", "h5py")
#
# for (pkg in required_packages) {
#   if (!(pkg %in% installed_packages)) {
#     conda_install("r-reticulate", pkg)
#   }
# }

# If this doesn't install 'tensorflow', go ahead and download anaconda from "https://www.anaconda.com/", open it up, go to environment and choose "r-reticulate". Then install the packages " "pandas", "numpy", "tensorflow", "h5py" " from the UI.
# Error associated with this fix:
# 'InvalidArchiveError("Error with archive C:\\Users\\ [...] \\\compose_set_interface.h.inc'")'
```

```{r}
df <- read_csv("../data/data_clean.csv", show_col_types = FALSE)

df_lag <- read_csv("../data/data_clean_lag.csv", show_col_types = FALSE)

df$Umsatz <- round(df$Umsatz)
df_lag$Umsatz <- round(df_lag$Umsatz)
```


```{r}
# Use the created Python environment
use_condaenv("r-reticulate", required = TRUE)

```

Splitting the data set into Training, Validation and Test data sets.
The test data set is fixed, as the time frame we want to predict is a given parameter. For the Training and validation sets, we chose roughly an 80 - 20 distribution.
```{r include=FALSE}

train_data <- subset(df, Datum >= "2013-07-01" & Datum <= "2018-03-31")

validation_data <- subset(df, Datum >= "2018-04-01" & Datum <= "2019-06-08")

test_data <- subset(df, Datum >= "2019-06-09" & Datum <= "2019-07-30")

# Check the dimensions of the data sets
cat("Training dataset dimensions:", dim(train_data), "\n")
cat("Validation dataset dimensions:", dim(validation_data), "\n")
cat("Test dataset dimensions:", dim(test_data), "\n")

```

```{r}
#
training_features <- as_tibble(model.matrix(Umsatz ~ as.factor(Warengruppe) +  as.factor(Wochentag) + as.factor(Monat) + Datum + Bewoelkung + Temperatur + as.logical(KielerWoche) + as.logical(FerienSH) + Windgeschwindigkeit, train_data))

validation_features <- as_tibble(model.matrix(Umsatz ~ as.factor(Warengruppe) +  as.factor(Wochentag) + as.factor(Monat) + Datum + Bewoelkung + Temperatur + as.logical(KielerWoche) + as.logical(FerienSH) + Windgeschwindigkeit, validation_data))

test_features <- as_tibble(model.matrix(Umsatz ~ as.factor(Warengruppe) +  as.factor(Wochentag) + as.factor(Monat) + Datum + Bewoelkung + Temperatur + as.logical(KielerWoche) + as.logical(FerienSH) + Windgeschwindigkeit, test_data))
#

training_label <- tibble(label=train_data$Umsatz)

validation_label <- tibble(label=validation_data$Umsatz)

test_label <- tibble(label=test_data$Umsatz)

# Since the Test data set is ranging only 1,5 months, we have asynchronous data frame dimensions. We add the following columns to allow the model to predict those values here as well.

test_features$`as.factor(Warengruppe)6` <- 0

test_features$`as.factor(Monat)2` <- 0
test_features$`as.factor(Monat)3` <- 0
test_features$`as.factor(Monat)4` <- 0
test_features$`as.factor(Monat)5` <- 0
test_features$`as.factor(Monat)6` <- ifelse(test_features$`as.factor(Monat)7` == 0, 1, 0)
test_features$`as.factor(Monat)8` <- 0
test_features$`as.factor(Monat)9` <- 0
test_features$`as.factor(Monat)10` <- 0
test_features$`as.factor(Monat)11` <- 0
test_features$`as.factor(Monat)12` <- 0

```

```{r}
# Check the dimensions of the dataframes
cat("Training features dimensions:", dim(training_features), "\n")
cat("Validation features dimensions:",
    dim(validation_features),
    "\n")
cat("Test features dimensions:", dim(test_features), "\n")
cat("\n")
cat("Training label dimensions:", dim(training_label), "\n")
cat("Validation label dimensions:", dim(validation_label), "\n")
cat("Test label dimensions:", dim(test_label), "\n")


```


Now let's go on to train the Neural Network!
Definition of the NN
```{python include=FALSE}

# Import needed Python libraries and functions
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam

# The argument "input_shape" for the definition of the input layer must include the number input variables (features) used for the model. To automatically calculate this number, we use the  function `r.training_features.keys()`, which returns the list of variable names of the dataframe `training_features`. Then, the function `len()` returns the length of this list of variable names (i.e. the number of variables in the input).

model_1 = Sequential([
  InputLayer(input_shape=(len(r.training_features.keys()), )),
  BatchNormalization(),
  Dense(32, activation='tanh'),
  Dense(8, activation='relu'),
  Dense(1)
])

```


### Training the Neural Network via the fit() function
```{python}

# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
 model_1.compile(loss="mse", optimizer=Adam(learning_rate=0.001))

# Schaetzung des Modells
history_1 = model_1.fit(r.training_features, r.training_label, epochs=1200,
                    validation_data = (r.validation_features, r.validation_label), verbose=0)


model_1.save("../ML_models/mod_nn_25.h5")
```

### Auswertung der Modelloptimierung
```{r}

# Grafische Ausgabe der Modelloptimierung
data_1 <- data.frame(val_loss = unlist(py$history_1$history$val_loss),
                  loss = unlist(py$history_1$history$loss))


# Plot
ggplot(data_1[-(1:10),]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss")

```


### Auswertung der Schätzergebnisse ###
```{r}

# Schätzung der (normierten) Preise für die Trainings- und Testdaten
training_predictions <- py$model_1$predict(training_features)
validation_predictions <- py$model_1$predict(validation_features)

temp <- test_features

test_predictions <- py$model_1$predict(test_features)
head(test_predictions)

# Vergleich der Gütekriterien für die Traingings- und Testdaten
cat(paste0("MAPE on the Training Data:\t", format(mape(training_label[[1]], training_predictions), digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Validation Data:\t", format(mape(validation_label[[1]], validation_predictions), digits=3, nsmall=2)))

```


```{r}
library(httr)

prediction_temp <- read.csv("../data/prediction_template.csv")

predictions <- data.frame(test_predictions)

prediction_temp <- select(prediction_temp, -Umsatz)

prediction_temp$predicted <- predictions$test_predictions

prediction_temp <- prediction_temp %>%
  rename(Umsatz = predicted)

name <- "Gruppe 11"

# Execution of the request
r <- POST("https://bakery-sales-mape-tolicqztoq-ey.a.run.app/", 
          body = list(name=name, predictions=prediction_temp),
          encode = "json")

# Output of MAPE in Percent
content(r, "parsed", "application/json")
```

